# vocabulary_loader.py
import streamlit as st
import os
from pathlib import Path

from data_config import VOCAB_FILE_PATH_2015, VOCAB_FILE_PATH_2022

def load_moe_vocabulary(file_path: str, year: str) -> set:
    """êµìœ¡ë¶€ ê¸°ë³¸ ì–´íœ˜ ëª©ë¡ íŒŒì¼ì„ ì½ì–´ ë‹¨ì–´ ì§‘í•©ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    vocabulary_set = set()
    
    try:
        # íŒŒì¼ ê²½ë¡œë¥¼ Path ê°ì²´ë¡œ ë³€í™˜
        file_path = Path(file_path)
        
        if not file_path.exists():
            st.warning(f"êµìœ¡ë¶€ ê¸°ë³¸ ì–´íœ˜ íŒŒì¼ '{file_path}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.info(f"íŒŒì¼ ìœ„ì¹˜ í™•ì¸: {file_path.absolute()}")
            return set()

        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = file_path.stat().st_size
        if file_size == 0:
            st.warning(f"{year}ë…„ ì–´íœ˜ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return set()

        with open(file_path, 'r', encoding='utf-8') as f:
            line_count = 0
            for line in f:
                line = line.strip()
                if not line:
                    continue
                    
                line_count += 1
                
                # í˜•ì‹: "word : meaning, meaning"
                if ' : ' in line:
                    parts = line.split(' : ', 1)
                    if len(parts) >= 1:
                        word = parts[0].strip()
                        if word and word.replace('.', '').replace('-', '').isalpha():
                            vocabulary_set.add(word.lower())
                else:
                    # êµ¬ë¶„ìê°€ ì—†ëŠ” ê²½ìš° ì²« ë²ˆì§¸ ë‹¨ì–´ë§Œ ì¶”ì¶œ
                    words = line.split()
                    if words:
                        word = words[0].strip()
                        if word and word.replace('.', '').replace('-', '').isalpha():
                            vocabulary_set.add(word.lower())
        
        if vocabulary_set:
            st.success(f"{year}ë…„ êµìœ¡ë¶€ ê¸°ë³¸ ì–´íœ˜ ëª©ë¡ ({len(vocabulary_set)}ê°œ) ë¡œë“œ ì™„ë£Œ")
        else:
            st.warning(f"{year}ë…„ ì–´íœ˜ íŒŒì¼ì—ì„œ ìœ íš¨í•œ ì–´íœ˜ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì²˜ë¦¬ëœ ì¤„ ìˆ˜: {line_count})")
            
    except UnicodeDecodeError:
        try:
            # UTF-8ì´ ì•ˆ ë˜ë©´ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
            with open(file_path, 'r', encoding='cp949') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    if ' : ' in line:
                        parts = line.split(' : ', 1)
                        if len(parts) >= 1:
                            word = parts[0].strip()
                            if word and word.replace('.', '').replace('-', '').isalpha():
                                vocabulary_set.add(word.lower())
            st.success(f"{year}ë…„ êµìœ¡ë¶€ ê¸°ë³¸ ì–´íœ˜ ëª©ë¡ ({len(vocabulary_set)}ê°œ) ë¡œë“œ ì™„ë£Œ (CP949 ì¸ì½”ë”©)")
        except Exception as e:
            st.error(f"{year}ë…„ êµìœ¡ë¶€ ê¸°ë³¸ ì–´íœ˜ ëª©ë¡ ë¡œë“œ ì¤‘ ì¸ì½”ë”© ì˜¤ë¥˜: {e}")
            
    except Exception as e:
        st.error(f"{year}ë…„ êµìœ¡ë¶€ ê¸°ë³¸ ì–´íœ˜ ëª©ë¡ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.info(f"íŒŒì¼ ê²½ë¡œ: {file_path.absolute()}")
        
    return vocabulary_set

def load_combined_moe_vocabulary() -> dict:
    """2015ë…„ê³¼ 2022ë…„ êµìœ¡ë¶€ ê¸°ë³¸ ì–´íœ˜ë¥¼ ëª¨ë‘ ë¡œë“œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    result = {
        "2015": set(),
        "2022": set(),
        "combined": set()
    }
    
    st.info("ğŸ“š êµìœ¡ë¶€ ê¸°ë³¸ ì–´íœ˜ íŒŒì¼ ë¡œë“œ ì¤‘...")
    
    # 2015ë…„ ì–´íœ˜ ë¡œë“œ
    vocab_2015 = load_moe_vocabulary(VOCAB_FILE_PATH_2015, "2015")
    result["2015"] = vocab_2015
    
    # 2022ë…„ ì–´íœ˜ ë¡œë“œ  
    vocab_2022 = load_moe_vocabulary(VOCAB_FILE_PATH_2022, "2022")
    result["2022"] = vocab_2022
    
    # í†µí•© ì–´íœ˜ (í•©ì§‘í•©)
    result["combined"] = vocab_2015.union(vocab_2022)
    
    if result["combined"]:
        st.info(f"""
**ğŸ“Š êµìœ¡ë¶€ ê¸°ë³¸ ì–´íœ˜ ë¡œë“œ ì™„ë£Œ:**
- **2015ë…„**: {len(result['2015'])}ê°œ
- **2022ë…„**: {len(result['2022'])}ê°œ  
- **í†µí•©**: {len(result['combined'])}ê°œ (ì¤‘ë³µ ì œê±°)
- **ê³µí†µ ì–´íœ˜**: {len(vocab_2015.intersection(vocab_2022))}ê°œ
- **2015ë…„ ê³ ìœ **: {len(vocab_2015 - vocab_2022)}ê°œ
- **2022ë…„ ê³ ìœ **: {len(vocab_2022 - vocab_2015)}ê°œ
        """)
    else:
        st.error("âš ï¸ ì–´íœ˜ íŒŒì¼ì„ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œì™€ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        # ë””ë²„ê¹… ì •ë³´ ì œê³µ
        st.info(f"""
**ğŸ” ë””ë²„ê¹… ì •ë³´:**
- 2015ë…„ íŒŒì¼ ê²½ë¡œ: `{VOCAB_FILE_PATH_2015}`
- 2022ë…„ íŒŒì¼ ê²½ë¡œ: `{VOCAB_FILE_PATH_2022}`
- í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: `{Path.cwd()}`

**ğŸ’¡ í•´ê²° ë°©ë²•:**
1. `data` í´ë”ê°€ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸
2. ì–´íœ˜ íŒŒì¼ì´ ì˜¬ë°”ë¥¸ ì´ë¦„ìœ¼ë¡œ `data` í´ë” ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
3. íŒŒì¼ ë‚´ìš©ì´ `word : meaning` í˜•ì‹ì¸ì§€ í™•ì¸
        """)
    
    return result

def get_vocabulary_for_grade(grade_level: str, all_vocabularies: dict) -> set:
    """í•™ë…„ì— ë”°ë¼ ì ì ˆí•œ ì–´íœ˜ ì§‘í•©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if grade_level == "ê³ 1":
        # ê³ 1ì€ 2022 ê°œì • êµìœ¡ê³¼ì •
        return all_vocabularies.get("2022", set())
    elif grade_level in ["ê³ 2", "ê³ 3"]:
        # ê³ 2, ê³ 3ëŠ” 2015 ê°œì • êµìœ¡ê³¼ì •
        return all_vocabularies.get("2015", set())
    else:
        # ê¸°íƒ€ì˜ ê²½ìš° í†µí•© ì–´íœ˜ ì‚¬ìš©
        return all_vocabularies.get("combined", set())

def analyze_vocabulary_level(text: str, target_vocab: set, all_vocabularies: dict) -> dict:
    """í…ìŠ¤íŠ¸ì˜ ì–´íœ˜ ìˆ˜ì¤€ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
    import re
    
    words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
    unique_words = set(words)
    
    # ëŒ€ìƒ ì–´íœ˜ ì§‘í•©ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
    if not target_vocab:
        st.warning("ëŒ€ìƒ ì–´íœ˜ ì§‘í•©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì–´íœ˜ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return {
            "total_unique_words": len(unique_words),
            "target_vocab_words": 0,
            "non_target_vocab_words": len(unique_words),
            "target_vocab_ratio": 0,
            "vocab_2015_words": 0,
            "vocab_2022_words": 0,
            "vocab_2015_ratio": 0,
            "vocab_2022_ratio": 0,
            "non_target_examples": sorted(list(unique_words))[:10]
        }
    
    # ëŒ€ìƒ ì–´íœ˜ ì§‘í•© ê¸°ì¤€ ë¶„ì„
    target_vocab_words = unique_words.intersection(target_vocab)
    non_target_vocab_words = unique_words - target_vocab
    
    # 2015/2022 ê°ê° ê¸°ì¤€ ë¶„ì„
    vocab_2015_words = unique_words.intersection(all_vocabularies.get("2015", set()))
    vocab_2022_words = unique_words.intersection(all_vocabularies.get("2022", set()))
    
    return {
        "total_unique_words": len(unique_words),
        "target_vocab_words": len(target_vocab_words),
        "non_target_vocab_words": len(non_target_vocab_words),
        "target_vocab_ratio": len(target_vocab_words) / len(unique_words) if unique_words else 0,
        "vocab_2015_words": len(vocab_2015_words),
        "vocab_2022_words": len(vocab_2022_words),
        "vocab_2015_ratio": len(vocab_2015_words) / len(unique_words) if unique_words else 0,
        "vocab_2022_ratio": len(vocab_2022_words) / len(unique_words) if unique_words else 0,
        "non_target_examples": sorted(list(non_target_vocab_words))[:10]  # ìµœëŒ€ 10ê°œ ì˜ˆì‹œ
    }

def debug_vocabulary_files():
    """ì–´íœ˜ íŒŒì¼ ë””ë²„ê¹…ì„ ìœ„í•œ í•¨ìˆ˜"""
    st.subheader("ğŸ” ì–´íœ˜ íŒŒì¼ ë””ë²„ê¹…")
    
    files_info = [
        ("2015ë…„", VOCAB_FILE_PATH_2015),
        ("2022ë…„", VOCAB_FILE_PATH_2022)
    ]
    
    for year, file_path in files_info:
        file_path = Path(file_path)
        
        st.write(f"**{year} íŒŒì¼:**")
        st.write(f"- ê²½ë¡œ: `{file_path}`")
        st.write(f"- ì ˆëŒ€ ê²½ë¡œ: `{file_path.absolute()}`")
        st.write(f"- ì¡´ì¬ ì—¬ë¶€: {'âœ…' if file_path.exists() else 'âŒ'}")
        
        if file_path.exists():
            size = file_path.stat().st_size
            st.write(f"- í¬ê¸°: {size:,} bytes")
            
            # íŒŒì¼ ì²« ëª‡ ì¤„ ë¯¸ë¦¬ë³´ê¸°
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = [f.readline().strip() for _ in range(5)]
                    lines = [line for line in lines if line]
                    
                st.write("- ì²« 5ì¤„ ë¯¸ë¦¬ë³´ê¸°:")
                for i, line in enumerate(lines, 1):
                    st.code(f"{i}: {line}")
                    
            except Exception as e:
                st.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        else:
            st.error("íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")

# ëª¨ë“ˆ ë¡œë“œ ì‹œ ì–´íœ˜ ë°ì´í„° ì´ˆê¸°í™”
try:
    MOE_VOCABULARIES = load_combined_moe_vocabulary()
    # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ë³¸ ì–´íœ˜ ì§‘í•© (í†µí•© ë²„ì „)
    MOE_VOCABULARY = MOE_VOCABULARIES.get("combined", set())
except Exception as e:
    st.error(f"ì–´íœ˜ ë°ì´í„° ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    MOE_VOCABULARIES = {"2015": set(), "2022": set(), "combined": set()}
    MOE_VOCABULARY = set()